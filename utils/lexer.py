import sys
from enum import Enum
from dataclasses import dataclass

from .error import SHLError

class TokenType(Enum):
    LEFT_PAREN = "LEFT_PAREN"
    RIGHT_PAREN = "RIGHT_PAREN"
    LEFT_BRACE = "LEFT_BRACE"
    RIGHT_BRACE = "RIGHT_BRACE"
    COMMA = "COMMA"
    DOT = "DOT"
    MINUS = "MINUS"
    PLUS = "PLUS"
    SEMICOLON = "SEMICOLON"
    COLON = "COLON"
    SLASH = "SLASH"
    STAR = "STAR"
    BANG = "BANG"
    BANG_EQUAL = "BANG_EQUAL"
    EQUAL = "EQUAL"
    EQUAL_EQUAL = "EQUAL_EQUAL"
    GREATER = "GREATER"
    GREATER_EQUAL = "GREATER_EQUAL"
    LESS = "LESS"
    LESS_EQUAL = "LESS_EQUAL"
    IDENTIFIER = "IDENTIFIER"
    STRING = "STRING"
    NUMBER = "NUMBER"
    AND = "AND"
    CLASS = "CLASS"
    ELSE = "ELSE"
    FALSE = "FALSE"
    FN = "FN"
    FOR = "FOR"
    GLOBAL = "GLOBAL"
    IF = "IF"
    INT = "INT"
    IN = "IN"
    LOCAL = "LOCAL"
    NIL = "NIL"
    OR = "OR"
    DISPLAY = "DISPLAY"
    RETURN = "RETURN"
    PARENT = "PARENT"
    THIS = "THIS"
    TRUE = "TRUE"
    VAR = "VAR"
    WALRUS = "WALRUS"
    WHILE = "WHILE"
    EOF = "EOF"

@dataclass
class Token:
    type: TokenType
    lexeme: str
    line: int
    col_start: int | None
    col_end: int | None
    src_line: str | None

    def __str__(self):
        return f"{self.type.name} {self.lexeme} {self.line} {self.col_start} {self.col_end} {self.src_line}"

@dataclass
class Lexer:
    """
    A lexer that scans shlang source code and tokenizes lexemes.
    
    Attributes:
        tokens: list[Token] - The list of tokens generated by the lexer
    """ 
    tokens: list[Token]
    reserved_chars: str
    _keywords: dict[str, TokenType]
    _single_char_tokens: dict[str, TokenType]
    _two_char_tokens: dict[str, tuple[TokenType | None, str, TokenType]]

    def __init__(self):
        self.tokens = [] 
        self.reserved_chars = "(){};:,+-*/=<>!&|"
        
        self._keywords = {
            "and": TokenType.AND,
            "class": TokenType.CLASS,
            "else": TokenType.ELSE,
            "false": TokenType.FALSE,
            "for": TokenType.FOR,
            "fn": TokenType.FN,
            "global": TokenType.GLOBAL,
            "if": TokenType.IF,
            "in": TokenType.IN,
            "int": TokenType.INT,
            "local": TokenType.LOCAL,
            "nil": TokenType.NIL,
            "or": TokenType.OR,
            "display": TokenType.DISPLAY,
            "return": TokenType.RETURN,
            "parent": TokenType.PARENT,
            "this": TokenType.THIS,
            "true": TokenType.TRUE,
            "var": TokenType.VAR,
            "while": TokenType.WHILE,
            "eof": TokenType.EOF
        }

        self._single_char_tokens = {
            "(": TokenType.LEFT_PAREN,
            ")": TokenType.RIGHT_PAREN,
            "{": TokenType.LEFT_BRACE,
            "}": TokenType.RIGHT_BRACE,
            ",": TokenType.COMMA,
            ".": TokenType.DOT,
            "-": TokenType.MINUS,
            "+": TokenType.PLUS,
            ";": TokenType.SEMICOLON,
            "*": TokenType.STAR,
            "/": TokenType.SLASH,
            "\"": TokenType.STRING,
            "'": TokenType.STRING,
            ":": TokenType.COLON,
        }

        self._two_char_tokens = {
            "!": (TokenType.BANG, "=", TokenType.BANG_EQUAL),
            "=": (TokenType.EQUAL, "=", TokenType.EQUAL_EQUAL),
            "<": (TokenType.LESS, "=", TokenType.LESS_EQUAL),
            ">": (TokenType.GREATER, "=", TokenType.GREATER_EQUAL),
            ":": (TokenType.COLON, "=", TokenType.WALRUS)
        }

    def scan_tokens(self, src: str) -> list[Token]:
        """
        Scan the source code and generate tokens.
        
        Args:
            src: The source code to scan
            
        Returns:
            A list of tokens
        """
        try:
            # Preprocess the source to separate special characters
            preprocessed_src = self._preprocess_source(src)
            self.tokens = [self.tokenizer(lex) for lex in preprocessed_src.split() if lex.strip()]
        except IOError:
            print(f"Error reading file: {src}")
            sys.exit(2)
        return self.tokens
    
    def _preprocess_source(self, src: str) -> str:
        """
        Preprocess the source code to separate special characters and make tokenization easier.
        
        Args:
            src: The source code to preprocess
            
        Returns:
            Preprocessed source code with special characters separated by spaces
        """
        res: str = ""
        
        i = 0
        while i < len(src):
            char = src[i]
            
            if char in ['"', "'"]:
                quote = char
                start = i
                i += 1 

                while i < len(src) and src[i] != quote:
                    if src[i] == '\\' and i + 1 < len(src):
                        i += 2
                    else:
                        i += 1
                
                if i < len(src):
                    i += 1
                    res += " " + src[start:i] + " "
                else:
                    res += " " + src[start:] + " "
            
            elif char in self.reserved_chars:
                res += " " + char + " "
                i += 1
            
            elif char == '/' and i + 1 < len(src) and src[i + 1] == '/':
                comment_start = i
                i += 2

                while i < len(src) and src[i] != '\n':
                    i += 1 
                res += " " + src[comment_start:i] + " "
            
            else:
                res += char
                i += 1
        
        return res

    def _create_token(self, token_type: TokenType, lexeme: str, line: int = 1, 
                     col_start: int = 1, col_end: int | None = None, 
                     src_line: str | None = None) -> Token:
        """Helper method to create a token with consistent defaults"""
        if col_end is None:
            col_end = col_start + len(lexeme) - 1 
        return Token(
            type=token_type, 
            lexeme=lexeme, 
            line=line, 
            col_start=col_start, 
            col_end=col_end, 
            src_line=src_line
        )

    def tokenizer(self, lexeme: str) -> Token:
        """
        Determine the token type for a given lexeme.
        
        Args:
            lexeme: The lexeme to tokenize
            
        Returns:
            A Token object
        
        Raises:
            SHLError: If the lexeme cannot be tokenized
        """
        if lexeme.lower() in self._keywords:
            return self._create_token(self._keywords[lexeme.lower()], lexeme)
        
        if lexeme in self._single_char_tokens:
            return self._create_token(self._single_char_tokens[lexeme], lexeme)
        
        token = self._try_parse_two_char_token(lexeme)
        if token:
            return token
        
        if self._is_number(lexeme):
            return self._create_token(TokenType.NUMBER, lexeme)
        
        if self._is_string(lexeme):
            content = lexeme[1:-1] if len(lexeme) >= 2 else lexeme
            return self._create_token(TokenType.STRING, content)
        
        if self._is_identifier(lexeme):
            return self._create_token(TokenType.IDENTIFIER, lexeme)
        
        raise SHLError(f"Unexpected token: '{lexeme}'")
    
    def _try_parse_two_char_token(self, lexeme: str) -> Token | None:
        """
        Try to parse a two-character token.
        
        Args:
            lexeme: The lexeme to parse
            
        Returns:
            A Token object if successful, None otherwise
        """
        # len(2) tokens ("==", "!=", etc.)
        if len(lexeme) == 2 and lexeme[0] in self._two_char_tokens:
            first_char, expected_second, two_char_type = self._two_char_tokens[lexeme[0]]
            if first_char and expected_second and two_char_type:
                if lexeme[1] == expected_second:
                    return self._create_token(two_char_type, lexeme)
        
        # len(1) tokens ("=", "!", etc.)
        if len(lexeme) == 1 and lexeme in self._two_char_tokens:
            first_char_type = self._two_char_tokens[lexeme][0]
            if first_char_type:
                return self._create_token(first_char_type, lexeme)

        # TODO: Determine if this should raise a TokenError or fail silently
        return None
    
    def _is_number(self, lexeme: str) -> bool:
        """
        Check if the lexeme is a valid number (integer or float).
        
        Args:
            lexeme: The lexeme to check
            
        Returns:
            True if the lexeme is a valid number, False otherwise
        """
        if lexeme.isdigit():
            return True
        
        if '.' in lexeme:
            parts = lexeme.split('.')
            if len(parts) == 2 and all(part.isdigit() or part == '' for part in parts):
                return True
        
        return False
    
    def _is_string(self, lexeme: str) -> bool:
        """
        Check if the lexeme is a valid string literal.
        
        Args:
            lexeme: The lexeme to check
            
        Returns:
            True if the lexeme is a valid string literal, False otherwise
        """
        return ((lexeme.startswith('"') and lexeme.endswith('"')) or 
                (lexeme.startswith("'") and lexeme.endswith("'")))
    
    def _is_identifier(self, lexeme: str) -> bool:
        """
        Check if the lexeme is a valid identifier.
        An identifier starts with a letter or underscore,
        followed by any number of letters, digits, or underscores.
        
        Args:
            lexeme: The lexeme to check
            
        Returns:
            True if the lexeme is a valid identifier, False otherwise
        """
        if not lexeme:
            return False
        
        if not (lexeme[0].isalpha() or lexeme[0] == '_'):
            return False
        
        for char in lexeme[1:]:
            if not (char.isalnum() or char == '_'):
                return False
        
        return True
